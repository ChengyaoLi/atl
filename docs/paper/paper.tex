% DOCUMENT CLASS
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts{}
\overrideIEEEmargins{}  % required to meet printer requirements.

% DOCUMENT STRUCTURE
\usepackage{subfiles}

% TABLES
\usepackage{caption}
\usepackage{multirow}

% ALGORITHMS
\usepackage{algorithmic}
\usepackage{algorithm}

% GRAPHICS
\usepackage{float}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{tikzscale}
%\usepackage{tkzgraph}
\usetikzlibrary{matrix}
\usepackage{verbatim}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{calc}
\usepackage{graphicx}
\usepackage{hyperref}

% MATH SYMBOLS
\usepackage{amsfonts} % mathbb{R}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}

% COMMENT BLOCKS
\usepackage{verbatim}

% SUBSCRIPT COMMANDS
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_\textrm{#1}}}

% TITLE
\title{\large \textbf Autonomous Quadrotor Landing on a Moving Platform in
Outdoor Environments}

% AUTHORS
\author{\
Chris Choi\authorrefmark{1}, Stanley Brown\authorrefmark{2} and  Steven L. Waslander\authorrefmark{3}
\thanks{\superscript{*} M.A.Sc. Candidate, Mechanical and Mechatronics Engineering, University of Waterloo; c33choi@uwaterloo.ca}
\thanks{\superscript{2} M.A.Sc. Candidate, Mechanical and Mechatronics Engineering, University of Waterloo; s52brown@uwaterloo.ca}
\thanks{\superscript{\dag} Assistant Professor, Mechanical and Mechatronics Engineering, University of Waterloo; stevenw@uwaterloo.ca}
\vspace{0.5in}
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

% ABSTRACT
\begin{abstract}
\end{abstract}


% INTRODUCTION
\section{Introduction}

% RELATED WORK
\section{Related Work}


% CONTRIBUTION
\section{Contribution}

\begin{itemize}
  \item{Robust Landing}
  \item{AprilTag estimation with Kalman Filter}
  \item{AprilTag windowing to speed up detection}
  \item{Light Invariant AprilTag detection}
  \item{Tracking Controller}
  \item{Trajectory Planning}
\end{itemize}

% PROBLEM FORMULATION
\section{Problem Formulation}



% TARGET TRACKING
\section{Target Tracking}
A popular visual fiducial system called AprilTag~\cite{Olson2011} was chosen,
the system allows us to identify the landing target along with the full 6 dof
estimation. There exists multiple implementations of
AprilTag,~\cite{AprilTagMIT} was chosen because it is the most used
implementation. In the following we describe steps taken to optimize the
AprilTag detection in-order to run the processing at a higher rate.


\subsection{Adaptive Image Processing}
The standard implementation of the AprilTag library when processing images
of 640 by 480 pixels results with an update rate of approximately 3 to
5 fps on the on-board computer too low for quadrotor controls. To improve
performance we adapted the image size depending on the estimated
distance between camera and AprilTag.

\subsection{AprilTag Windowing}
In~\cite{Ling2014} an attempt was made to optimize the AprilTag library
by reducing the brightness of the image such that the majority of the
image is black, we went one step further and masked the lastest image
using the last detected AprilTag camera coordinates, rendering everything
other than the AprilTag black, with this modification we assumed the image
coordinates between frames did not vary too wildly, else the mask is
removed and the full image is inputed to the AprilTag detector for
processing.

Once the center of the AprilTag in camera frame is found the top left and
bottom right corners of the AprilTag can be calculated with:

\begin{align}
  X_{\text{top left}} &= X - (l / 2) - X_{\text{padding}} \\
  Y_{\text{top left}} &= Y - (l / 2) - Y_{\text{padding}} \\
  X_{\text{bottom right}} &= X + (l / 2) + X_{\text{padding}} \\
  Y_{\text{bottom right}} &= Y + (l / 2) + Y_{\text{padding}}
\end{align}

Where $X, Y$ are positions of the AprilTag in the camera frame,
$X_{\text{padding}}, Y_{\text{padding}}$ are the mask padding in the camera
frame, $l$ is the AprilTag side length in meters, and finally $X_{\text{top
left}}, Y_{\text{top left}}, X_{\text{bottom right}}, Y_{\text{bottom right}}$
represent the top left and bottom right corners in the camera frame. Adding the
mask padding at this point allows the padding to be defined in the camera
frame which increases and decreases proportionally in the image frame
depending on the depth-distance between camera and AprilTag.

Using the pin-hole camera model, we can convert the AprilTag's corners
from camera frame into image frame with the following:

\begin{equation}
  x = \dfrac{f_{x} * X}{Z}
\end{equation}

\begin{equation}
  y = \dfrac{f_{y} * Y}{Z}
\end{equation}

Where $f_{x}$ and $f_{y}$ are the focal length in pixels in the $x$ and
$y$ axis, and finally $x, y$ are image pixel coordinates. Using the
image coordinates of the top left and bottom right corners of the AprilTag
a mask can be created.

\subsection{Nested AprilTag}
To address visibility issues of the AprilTag as the field of view of is reduced
during landing, we found a unique combination of AprilTag ids that allows us to
place a smaller AprilTag in the center of a larger AprilTag (see
Fig~\ref{fig:apriltag_illumination_invariant}). By shear coincidence the
AprilTag implementation~\cite{AprilTagMIT} always prefers the smaller secondary
tag when both are detected.

\subsection{Illumination Invariant AprilTag}
A common issue in computer vision is illumination changes in the
environment which in turn interferes with the performance of detection
algorithms. From our experience the standard black and white AprilTag
fails to be detected if a shadow is casted partially or fully upon the tag
features, this is because the AprilTag detection relies heavily on the
edges and lines of the tag to be able to estimate the 6dof pose.

Reliable image processing regardless of the time of day is vital to the
success of target detection. Using methods described in~\cite{Maddern14}
we have idenitified the best colours for an illumination invariant
AprilTag to be blue and green (see
Fig~\ref{apriltag_illumination_invariant}), additionally

\begin{equation}
  I = \log(R_{2}) - \alpha \log(R_{1}) - (1 - \alpha) \log(R_{3})
\end{equation}

Where $R_{1}, R_{2}, R_{3}$ are sensor responses (or image channels)
corresponding to peak sensitivies at ordered wavelengths $\lambda_{1}
< \lambda_{2} < \lambda_{3}$.


\begin{figure}
\label{fig:apriltag_illumination_invariant}
  \includegraphics[width=\linewidth]{images/apriltag_illumination_invariant}
  \caption{Illumination Invariant AprilTag}
\end{figure}


% TARGET ESTIMATION
\section{Target Estimation}
The inertial target position, linear velocity, angular velocity and
heading are estimated with an Extended Kalman Filter running at 200Hz, the
estimation is in turn used for tracking and landing controllers on board
the quadrotor.

\subsection{Process Model}
We chose to use a two-wheel robot motion model to approximate the forward
kinematics of the target, which is given as:

\begin{equation}
    \begin{split}
        \textbf{x}
        &= \begin{bmatrix}
            x_{1} & x_{2} & x_{3} & x_{4} & x_{5} & x_{6} & x_{7}
        \end{bmatrix} \\
        &=
        \begin{bmatrix}
            x & y & z & \theta & v & \omega & v_{z}
        \end{bmatrix}
    \end{split}
\end{equation}

\begin{equation}
    \textbf{u}(t)
        = \begin{bmatrix}
            u_{1} \\
            u_{2} \\
            u_{3}
        \end{bmatrix}
        = \begin{bmatrix}
            v \\
            \omega \\
            v_{z}
        \end{bmatrix}
        = \begin{bmatrix}
            \mathcal{N}_{v} \\
            \mathcal{N}_{\omega} \\
            \mathcal{N}_{v_{z}}
        \end{bmatrix}
\end{equation}

\begin{equation}
    \dot{\textbf{x}} = \begin{bmatrix}
        \dot{x_{1}} \\ 
        \dot{x_{2}} \\ 
        \dot{x_{3}} \\ 
        \dot{x_{4}} \\
        \dot{x_{5}} \\
        \dot{x_{6}} \\
        \dot{x_{7}}
    \end{bmatrix} =
    \begin{bmatrix}
        x_{5} \cos(x_{4}) \\
        x_{5} \sin(x_{4}) \\
        x_{7} \\
        x_{6} \\
        u_{1} \\
        u_{2} \\
        u_{3} \\
    \end{bmatrix}
\end{equation}

Where the estimator estimates the inertial position in $x$, $y$ and $z$
direction, heading $\theta$, wheel velocity $v$, steering angular velocity
$\omega$ and linear velocity $v_{z}$ in the $z$ direction. It is worth
noting the linear velocity in the $x$ and $y$ direction can be calculated
from $v \cos(\theta)$ and $v \sin(\theta)$ respectively. The inputs to the
process model are driven by Gaussian White Noise, since we do not possess
input information to the two wheel robot motion model.

\subsection{Measurement Model}

\begin{equation}
  h(t) = \begin{bmatrix} p_{1 \times 3} & {0}_{1 \times 4} \end{bmatrix}
\end{equation}


% GIMBAL TRANSFORMS
\section{Gimbal Transforms}

Transform detected target from image frame to gimbal joint frame

\begin{equation}
  x_{\text{gjf}} = R^{\text{gjf}}_{\text{i}}
    R^{\text{nwu}}_{\text{cf}}
    x_{\text{i}} + t_{c}
\end{equation}

Gimbal joint frame to body planar frame

\begin{equation}
  x_{\text{bpf}} = R^{\text{bpf}}_{\text{gjf}} x_{\text{gjf}}
\end{equation}

Body planar frame to inertial frame

\begin{equation}
  x_{\text{if}} = R^{\text{enu}}_{\text{nwu}}
    R^{\text{if}}_{\text{bpf}}
    x_{\text{bpf}}
\end{equation}



% TRAJECTORY PLANNING
\section{Trajectory Planning}

\begin{equation}
  \begin{split}
    \textbf{x} 
      &= \begin{bmatrix}
        x_{1} & x_{2} & x_{3} & x_{4}
      \end{bmatrix}^{T} \\ 
      &= \begin{bmatrix}
        x & \dot{x} & z & \dot{z}
      \end{bmatrix}^{T} \in {\rm I\!R}^{4}
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \textbf{x}_{0} = \begin{bmatrix}
        x_{0} & \dot{x_{d}} & z_{d} & \dot{z_{d}}
    \end{bmatrix}^{T} \\
    \textbf{x}_{d} = \begin{bmatrix}
        x_{d} & \dot{x_{d}} & z_{d} & \dot{z_{d}}
    \end{bmatrix}^{T}
  \end{split}
\end{equation}

\begin{equation}
  \textbf{u}(t)
    = \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix}
    = \begin{bmatrix} a_{z} \\ \theta \end{bmatrix}
    \in {\rm I\!R}^{2}
\end{equation}

\begin{equation}
  \dot{\textbf{x}} =
    \begin{bmatrix}
      \dot{x_{1}} \\ \dot{x_{2}} \\ \dot{x_{3}} \\ \dot{x_{4}}
    \end{bmatrix} =
    f(\textbf{x}, \textbf{u}) =
    \begin{bmatrix}
      x_{2} \\
      u_{1} \sin(u_{2}) \\
      x_{4} \\
      u_{1} \cos(u_{2}) - g \\
      u_{2}
    \end{bmatrix}
\end{equation}

\begin{align} 
  \text{minimize} \quad &J(\textbf{x}, \textbf{u}) \\
  \text{s.t.} \quad & \dot{\textbf{x}} = f(\textbf{x}, \textbf{u}) \\
    & \textbf{x}(t = 0) = \textbf{x}_{0} \\
    & \textbf{x}(t = t_{f}) = \textbf{x}_{d} \\
    & \textbf{u} \in \textbf{U} \quad \forall t \in [0, t_{f}]
\end{align}


% QUADROTOR CONTROL
\section{Quadrotor Control}
For target tracking a Proportional-Integral-Derivative (PID) controller
was implemented. The estimated target position in inertial frame is
converted to body frame and used as control errors to the tracker control




% EXPERIMENT RESULTS
\section{Experiment Results}



% CONCLUSIONS
\section{Conclusions and Future Work}

% BIBLIOGRAPHY
\bibliography{paper}
\bibliographystyle{ieeetr}

\end{document}
